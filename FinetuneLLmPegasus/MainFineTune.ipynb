{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Mistral-7B Language Model Fine-Tuning Script\n",
    "\n",
    "This script fine-tunes the Mistral-7B language model on a custom dataset. It uses the Hugging Face \n",
    "Transformers library to load the pre-trained model and tokenizer, prepare the dataset, and perform \n",
    "the fine-tuning process.\n",
    "\n",
    "Key features:\n",
    "1. Loads and prepares data from a JSON file\n",
    "2. Uses the Mistral-7B model and tokenizer\n",
    "3. Tokenizes the dataset for training\n",
    "4. Configures training arguments\n",
    "5. Performs fine-tuning using the Hugging Face Trainer\n",
    "6. Saves the fine-tuned model\n",
    "\n",
    "The script is designed to work with instruction-response pairs stored in a JSON file. It formats \n",
    "the data, tokenizes it, and then fine-tunes the model on this dataset.\n",
    "\n",
    "Note: This script requires access to the Mistral-7B model, which may need authentication. \n",
    "Make sure you have the necessary permissions and have set up your Hugging Face token correctly.\n",
    "\n",
    "Usage:\n",
    "Ensure you have a 'data.json' file in the same directory as this script, then run:\n",
    "python script_name.py\n",
    "\n",
    "Requirements:\n",
    "- transformers\n",
    "- datasets\n",
    "- torch (PyTorch)\n",
    "- Sufficient GPU memory to load and fine-tune the Mistral-7B model\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    LlamaTokenizer,\n",
    "    LlamaForCausalLM,\n",
    "    TrainingArguments,\n",
    "    Trainer\n",
    ")\n",
    "\n",
    "batch_size = 1024 # Adjust this based on your memory capacity\n",
    "\n",
    "# Load data from JSON file\n",
    "def load_data(file_path):\n",
    "    data = []\n",
    "    with open(file_path, 'r') as f:\n",
    "        items = json.load(f)\n",
    "        for item in items:\n",
    "            text = f\"Instruction: {item['instruction']}\\nResponse: {item['response']}\"\n",
    "            data.append(text)\n",
    "    return data\n",
    "\n",
    "# Prepare dataset\n",
    "def prepare_dataset(data):\n",
    "    return Dataset.from_dict({\"text\": data})\n",
    "\n",
    "def tokenize_data(examples):\n",
    "    tokenized_inputs = tokenizer(examples[\"text\"], truncation=True, padding=\"max_length\", max_length=1024)\n",
    "    tokenized_inputs[\"labels\"] = tokenized_inputs[\"input_ids\"].copy()\n",
    "    return tokenized_inputs\n",
    "\n",
    "# Load model and tokenizer\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-v0.3\", use_auth_token=\"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\")\n",
    "# Add padding token if not present\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.add_special_tokens({'pad_token': tokenizer.eos_token})\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-v0.3\", use_auth_token=\"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\")\n",
    "\n",
    "# Load and prepare data\n",
    "print(\"--> prepare the data\")\n",
    "data = load_data(\"data.json\")\n",
    "dataset = prepare_dataset(data)\n",
    "tokenized_dataset = dataset.map(tokenize_data, batched=True, batch_size=batch_size, num_proc=4)\n",
    "\n",
    "# Set up training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=4,\n",
    "    save_steps=10_000,\n",
    "    save_total_limit=2,\n",
    "    learning_rate=2e-5,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs', # directory for storing logs\n",
    "    logging_steps=200, # log every 200 steps\n",
    ")\n",
    "\n",
    "# Initialize Trainer\n",
    "print(\"--> init training\")\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    ")\n",
    "\n",
    "print(\"--> start training\")\n",
    "# Start training\n",
    "trainer.train()\n",
    "\n",
    "# Save the model\n",
    "print(\"--> saving the model\")\n",
    "trainer.save_model(\"./mistralai7B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Falcon-7B Language Model Fine-Tuning Script\n",
    "\n",
    "This script fine-tunes the Falcon-7B language model on a custom dataset. It uses the Hugging Face \n",
    "Transformers library to load the pre-trained model and tokenizer, prepare the dataset, and perform \n",
    "the fine-tuning process.\n",
    "\n",
    "Key features:\n",
    "1. Loads and prepares data from a JSON file\n",
    "2. Uses the Falcon-7B model and tokenizer\n",
    "3. Tokenizes the dataset for training\n",
    "4. Configures training arguments\n",
    "5. Performs fine-tuning using the Hugging Face Trainer\n",
    "6. Saves the fine-tuned model\n",
    "7. Supports CPU and MPS devices\n",
    "\n",
    "The script is designed to work with instruction-response pairs stored in a JSON file. It formats \n",
    "the data, tokenizes it, and then fine-tunes the model on this dataset.\n",
    "\n",
    "Usage:\n",
    "Ensure you have a 'data.json' file in the same directory as this script, then run:\n",
    "python script_name.py\n",
    "\n",
    "Requirements:\n",
    "- transformers\n",
    "- datasets\n",
    "- torch (PyTorch)\n",
    "- Sufficient GPU memory to load and fine-tune the Falcon-7B model\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling,\n",
    ")\n",
    "\n",
    "# Function to load data from JSON file\n",
    "def load_data(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    return data\n",
    "\n",
    "# Function to prepare dataset\n",
    "def prepare_dataset(data):\n",
    "    texts = [f\"Instruction: {item['instruction']}\\nResponse: {item['response']}\" for item in data]\n",
    "    return Dataset.from_dict({\"text\": texts})\n",
    "\n",
    "# Function to tokenize data\n",
    "def tokenize_data(examples, tokenizer):\n",
    "    model_inputs = tokenizer(examples[\"text\"], truncation=True, padding=\"max_length\", max_length=1024)\n",
    "    model_inputs[\"labels\"] = model_inputs[\"input_ids\"].copy()\n",
    "    return model_inputs\n",
    "\n",
    "# Initialize tokenizer and model\n",
    "def initialize_model_and_tokenizer(model_name):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    # Add padding token if not present\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.add_special_tokens({'pad_token': tokenizer.eos_token})\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name, attn_implementation=\"eager\")\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "    return tokenizer, model\n",
    "\n",
    "# Function to define the training arguments\n",
    "def define_training_args(output_dir=\"./results\"):\n",
    "    return TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        overwrite_output_dir=True,\n",
    "        num_train_epochs=1,\n",
    "        per_device_train_batch_size=2,\n",
    "        gradient_accumulation_steps=8,\n",
    "        save_steps=10_000,\n",
    "        save_total_limit=2,\n",
    "        learning_rate=2e-5,\n",
    "        warmup_steps=500,\n",
    "        weight_decay=0.01,\n",
    "        logging_dir='./logs',\n",
    "        logging_steps=200,\n",
    "        use_mps_device=True,\n",
    "    )\n",
    "\n",
    "# Function to define the Trainer\n",
    "def create_trainer(model, args, train_dataset, tokenizer):\n",
    "    data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "    return Trainer(\n",
    "        model=model,\n",
    "        args=args,\n",
    "        train_dataset=train_dataset,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "    )\n",
    "\n",
    "# Function to fine-tune and train the model\n",
    "def train_model(trainer, output_dir):\n",
    "    trainer.train()\n",
    "    trainer.save_model(output_dir)\n",
    "\n",
    "# Main function to orchestrate the training process\n",
    "def main():\n",
    "    # Set the device\n",
    "    device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # Initialize model and tokenizer\n",
    "    model_name = \"ybelkada/falcon-7b-sharded-bf16\"\n",
    "    tokenizer, model = initialize_model_and_tokenizer(model_name)\n",
    "\n",
    "    # Move model to the appropriate device\n",
    "    model.to(device)\n",
    "\n",
    "    # Load and prepare data\n",
    "    data = load_data(\"data.json\")\n",
    "    dataset = prepare_dataset(data)\n",
    "    tokenized_dataset = dataset.map(\n",
    "        lambda x: tokenize_data(x, tokenizer),\n",
    "        batched=True,\n",
    "        num_proc=4,\n",
    "        remove_columns=dataset.column_names\n",
    "    )\n",
    "\n",
    "    # Define training arguments\n",
    "    training_args = define_training_args()\n",
    "\n",
    "    # Create Trainer\n",
    "    trainer = create_trainer(model, training_args, tokenized_dataset, tokenizer)\n",
    "\n",
    "    # Train model\n",
    "    output_dir = \"./falcon7B\"\n",
    "    train_model(trainer, output_dir)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(prompts, model, tokenizer, device, temperature=0.7, max_length=200):\n",
    "    # Set the model to evaluation mode\n",
    "    model.eval()\n",
    "    # Initialize an empty list to store the generated texts\n",
    "    generated_texts = []\n",
    "\n",
    "    # Disable gradient calculations for inference\n",
    "    with torch.no_grad():\n",
    "        # Iterate over each prompt in the input prompts list\n",
    "        for prompt in tqdm(prompts, desc=\"Generating texts\"):\n",
    "            # Tokenize the prompt and move the input tensors to the specified device\n",
    "            inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "            # Generate text using the model with the specified max_length and temperature\n",
    "            output = model.generate(**inputs, max_length=max_length, temperature=temperature)\n",
    "            # Decode the generated tokens to a string, skipping special tokens\n",
    "            generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "            # Append the generated text to the list of generated texts\n",
    "            generated_texts.append(generated_text)\n",
    "    \n",
    "    # Return the list of generated texts\n",
    "    return generated_texts\n",
    "\n",
    "# Example usage of the generate_text function\n",
    "prompts = [\"create a complete pegasus workflow using python for machine learning\"]\n",
    "# Call the function with the prompts, model, tokenizer, and device\n",
    "generated_texts = generate_text(prompts, model, tokenizer, device)\n",
    "\n",
    "# Print each generated text\n",
    "for text in generated_texts:\n",
    "    print(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import LlamaTokenizer, LlamaForCausalLM\n",
    "\n",
    "# Specify the paths to the saved model and tokenizer\n",
    "model_path = \"./mistralai7B\"\n",
    "# tokenizer_path = \"pthornton614/CodeLama-7b-Instruct\"  # Alternate tokenizer path, commented out\n",
    "tokenizer_path = \"mistralai/Mistral-7B-v0.3\"  # Tokenizer path used during fine-tuning\n",
    "\n",
    "# Load the tokenizer\n",
    "print(\"Loading tokenizer...\")\n",
    "tokenizer = LlamaTokenizer.from_pretrained(tokenizer_path)\n",
    "\n",
    "# Set the padding token if it is not already set\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print(\"Tokenizer loaded.\")\n",
    "\n",
    "# Load the model\n",
    "print(\"Loading model...\")\n",
    "# Check if the Apple Silicon (MPS) backend is available, otherwise use CPU\n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "# Load the model and move it to the specified device\n",
    "model = LlamaForCausalLM.from_pretrained(model_path).to(device)\n",
    "print(\"Model loaded.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text...\n",
      "Generating text...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating sequences: 100%|██████████| 1/1 [01:12<00:00, 72.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text:\n",
      "Instruction: Convert the following SLURM script into a Pegasus workflow using the Pegasus.api Python package. The workflow should replicate the functionality of the SLURM script as closely as possible, including file handling, job execution, and cleanup. #!/bin/bash\n",
      "#SBATCH --job-name=array_job\n",
      "#SBATCH --output=array_job_%A_%a.out\n",
      "#SBATCH --error=array_job_%A_%a.err\n",
      "#SBATCH --array=0-9\n",
      "\n",
      "# Define and create a unique scratch directory\n",
      "SCRATCH_DIRECTORY=/global/work/${USER}/job-array-example/${SLURM_JOBID}\n",
      "mkdir -p ${SCRATCH_DIRECTORY}\n",
      "cd ${SCRATCH_DIRECTORY}\n",
      "\n",
      "cp ${SLURM_SUBMIT_DIR}/test.py ${SCRATCH_DIRECTORY}\n",
      "\n",
      "# Each job will see a different ${SLURM_ARRAY_TASK_ID}\n",
      "echo \"now processing task id:: ${SLURM_ARRAY_TASK_ID}\"\n",
      "python test.py > output_${SLURM_ARRAY_TASK_ID}.txt\n",
      "\n",
      "# After the job is done we copy our output back to ${SLURM_SUBMIT_DIR}\n",
      "cp output_${SLURM_ARRAY_TASK_ID}.txt ${SLURM_SUBMIT_DIR}\n",
      "\n",
      "# We step out of the scratch directory and remove it\n",
      "cd ${SLURM_SUBMIT_DIR}\n",
      "rm -rf ${SCRATCH_DIRECTORY}\n",
      "\n",
      "# Happy end\n",
      "exit 0\n",
      "\n",
      "Response: from Pegasus.api import *\n",
      "\n",
      "# Define workflow and job\n",
      "array_workflow = Workflow(\"array_workflow\")\n",
      "job = Job(\"process\")\n",
      "args = f\"--submit-directory={PEGASUS_SUBDIT_DIR} --scratch-directory={PEGASUS_SUBDIT_DIR} --workflow={WORKFLOW_NAME} --job-name={JOB_NAME} --output-directory={WORK_DIR} --input-files={TRAIN_DATA} --model-files={MODEL_DIR} --data-files={DATA_DIR}\".split()\n",
      "job.add_args(*args)\n",
      "array_workflow.add_job(job)\n",
      "\n",
      "# Write the workflow to a file\n",
      "array_workflow.write()\n",
      "\n",
      "# Submit the workflow to start\n",
      "submit_directory = f\"{HOME}\".replace(\"\\\\\", \"\\\\\\\\\")\n",
      "submit_directory = f\"/Volumes/{submit_directory}\".replace(\"\\\\\", \"\\\\\\\\\")\n",
      "submit_directory = f\"/Users/maciej/Work/{submit_directory}\".replace(\"\\\\\", \"\\\\\\\\\")\n",
      "submit_directory = \"/Volumes/Maciej/Work/\"\n",
      "\n",
      "mkdir_command = f\"mkdir -p ${submit_directory} ${HOME}/bin ${HOME}/.bosco\"\n",
      "\n",
      "mkdir_job = Job(mkdir_command)\n",
      "mkdir_job.submit()\n",
      "\n",
      "stash_home = f\"{submit_directory}StashHome\"\n",
      "\n",
      "transfer_job = Job(transfer_job_name, _id=TransferJobName)\n",
      "transfer_job.add_args(submit_directory, stash_home)\n",
      "transfer_job.add_inputs(files_glob)\n",
      "transfer_job.add_outputs(submit_directory, stash_home)\n",
      "transfer_job.submit()\n",
      "\n",
      "execute_workflow = Workflow(SubWorkFlow)\n",
      "execute_workflow.write()\n",
      "subworkflow_stager_job = Job(StageSubWorkFlowToStashHome)\n",
      "subworkflow_stager_job.add_args(submit_directory, stash_home)\n",
      "subworkflow_stager_job.add_inputs(TransferJobName, SubWorkflow.get_full_name())\n",
      "subworkflow_stager_job.add_outputs(submit_directory, stash_home)\n",
      "subworkflow_stager_job.submit()\n",
      "\n",
      "analyze_job = Job(SubWorkFlow, wait_time=1*60, output_dir=submit_directory, subworkflow_name=EXECUTABLE_NAME, use_condor_pool=True, require_grid_shifter=True)\n",
      "analyze_job.add_args(submit_directory, stash_home, files_glob\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "from transformers import LlamaTokenizer, LlamaForCausalLM\n",
    "\n",
    "# Specify the paths to the saved model and tokenizer\n",
    "model_path = \"./mistralai7B\"\n",
    "tokenizer_path = \"mistralai/Mistral-7B-v0.3\"  # Same as the path used during fine-tuning\n",
    "\n",
    "# Load the tokenizer\n",
    "print(\"Loading tokenizer...\")\n",
    "tokenizer = LlamaTokenizer.from_pretrained(tokenizer_path)\n",
    "\n",
    "# Set the padding token if it is not already set\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print(\"Tokenizer loaded.\")\n",
    "\n",
    "# Load the model\n",
    "print(\"Loading model...\")\n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "model = LlamaForCausalLM.from_pretrained(model_path).to(device)\n",
    "print(\"Model loaded.\")\n",
    "\n",
    "# Prepare a prompt\n",
    "slurm_script = \"\"\"#!/bin/bash\n",
    "#SBATCH --job-name=array_job\n",
    "#SBATCH --output=array_job_%A_%a.out\n",
    "#SBATCH --error=array_job_%A_%a.err\n",
    "#SBATCH --array=0-9\n",
    "\n",
    "# Define and create a unique scratch directory\n",
    "SCRATCH_DIRECTORY=/global/work/${USER}/job-array-example/${SLURM_JOBID}\n",
    "mkdir -p ${SCRATCH_DIRECTORY}\n",
    "cd ${SCRATCH_DIRECTORY}\n",
    "\n",
    "cp ${SLURM_SUBMIT_DIR}/test.py ${SCRATCH_DIRECTORY}\n",
    "\n",
    "# Each job will see a different ${SLURM_ARRAY_TASK_ID}\n",
    "echo \"now processing task id:: ${SLURM_ARRAY_TASK_ID}\"\n",
    "python test.py > output_${SLURM_ARRAY_TASK_ID}.txt\n",
    "\n",
    "# After the job is done we copy our output back to ${SLURM_SUBMIT_DIR}\n",
    "cp output_${SLURM_ARRAY_TASK_ID}.txt ${SLURM_SUBMIT_DIR}\n",
    "\n",
    "# We step out of the scratch directory and remove it\n",
    "cd ${SLURM_SUBMIT_DIR}\n",
    "rm -rf ${SCRATCH_DIRECTORY}\n",
    "\n",
    "# Happy end\n",
    "exit 0\n",
    "\"\"\"\n",
    "\n",
    "prompt = f\"Instruction: Convert the following SLURM script into a Pegasus workflow using the Pegasus.api Python package. The workflow should replicate the functionality of the SLURM script as closely as possible, including file handling, job execution, and cleanup. {slurm_script}\\nResponse:\"\n",
    "\n",
    "# Tokenize the prompt and provide attention mask\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True, max_length=1000)\n",
    "input_ids = inputs.input_ids.to(device)\n",
    "attention_mask = inputs.attention_mask.to(device)\n",
    "\n",
    "# Generate text\n",
    "print(\"Generating text...\")\n",
    "with torch.no_grad():\n",
    "    for _ in tqdm(range(1), desc=\"Generating sequences\"):\n",
    "        outputs = model.generate(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            max_length=1000,\n",
    "            num_return_sequences=1,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            top_k=50,\n",
    "            do_sample=True\n",
    "        )\n",
    "\n",
    "# Decode and print the generated text\n",
    "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(\"Generated text:\")\n",
    "print(generated_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"\"\"Instruction: Convert the following SLURM script into a Pegasus workflow using the Pegasus.api Python package. \n",
    "Your response should be a complete Python script that includes:\n",
    "1. Necessary imports from Pegasus.api\n",
    "2. Workflow creation\n",
    "3. Job definitions that replicate the SLURM script functionality\n",
    "4. Transformation, Site, and Replica catalogs as needed\n",
    "5. Writing the workflow to a file\n",
    "\n",
    "SLURM script:\n",
    "{slurm_script}\n",
    "\n",
    "Begin your response with 'from Pegasus.api import *' and end it with 'workflow.write()'.\n",
    "Do not repeat code unnecessarily. Ensure each part of the SLURM script functionality is addressed only once in the Pegasus workflow.\n",
    "\n",
    "Response:\n",
    "\"\"\"\n",
    "outputs = model.generate(\n",
    "    input_ids,\n",
    "    attention_mask=attention_mask,\n",
    "    max_length=1000,  # Increase max length for longer outputs\n",
    "    num_return_sequences=1,\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    "    do_sample=True,  # Enable sampling\n",
    "    temperature=0.2,  # Adjust temperature (lower for more focused outputs)\n",
    "    top_k=50,  # Limit to top k tokens\n",
    "    top_p=0.95,  # Nucleus sampling\n",
    "    no_repeat_ngram_size=3,  # Prevent repetition of 3-grams\n",
    "    early_stopping=True,\n",
    "    repetition_penalty=1.2  # Penalize repetition\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text:\n",
      "Instruction: Convert the following SLURM script into a Pegasus workflow using the Pegasus.api Python package. The workflow should replicate the functionality of the SLURM script as closely as possible, including file handling, job execution, and cleanup. #!/bin/bash\n",
      "#SBATCH --job-name=array_job\n",
      "#SBATCH --output=array_job_%A_%a.out\n",
      "#SBATCH --error=array_job_%A_%a.err\n",
      "#SBATCH --array=0-9\n",
      "\n",
      "# Define and create a unique scratch directory\n",
      "SCRATCH_DIRECTORY=/global/work/${USER}/job-array-example/${SLURM_JOBID}\n",
      "mkdir -p ${SCRATCH_DIRECTORY}\n",
      "cd ${SCRATCH_DIRECTORY}\n",
      "\n",
      "cp ${SLURM_SUBMIT_DIR}/test.py ${SCRATCH_DIRECTORY}\n",
      "\n",
      "# Each job will see a different ${SLURM_ARRAY_TASK_ID}\n",
      "echo \"now processing task id:: ${SLURM_ARRAY_TASK_ID}\"\n",
      "python test.py > output_${SLURM_ARRAY_TASK_ID}.txt\n",
      "\n",
      "# After the job is done we copy our output back to ${SLURM_SUBMIT_DIR}\n",
      "cp output_${SLURM_ARRAY_TASK_ID}.txt ${SLURM_SUBMIT_DIR}\n",
      "\n",
      "# We step out of the scratch directory and remove it\n",
      "cd ${SLURM_SUBMIT_DIR}\n",
      "rm -rf ${SCRATCH_DIRECTORY}\n",
      "\n",
      "# Happy end\n",
      "exit 0\n",
      "\n",
      "Response: from Pegasu.api import *\n",
      "\n",
      "wf = Workflow(\"converted-slurm\")\n",
      "\n",
      "for i in range(10):\n",
      "    job = Job(process)\n",
      "    input_files = File(File(job.get_input()[0]))\n",
      "    output_files.append(File(\"results_{}\".format(i)))\n",
      "    wf.add_jobs(job, args=[input_files])\n",
      "\n",
      "    # Adding files for each job\n",
      "    process_file = File(\"process.py\")\n",
      "    data_file     = File(\"/Volumes/ryan/data.csv\")\n",
      "                            .add_metadata(creator=\"ryan\", date=Date.today())\n",
      "    )\n",
      "    results_file   = File(\"./*\").add_regex_replica(\"local\", \"/Users/ryantanaka/Documents/Pegasus/[0]\".format(__FILE__))\n",
      "                                .add(\"remote\", \"/home/rytanaka/Work/Papajimmy/[1]\".replace(\"\\\\\", \"/\"))\n",
      "                              )\n",
      "                          );\n",
      "    submit_directory = Directory(submit_directory_path).add_args(\"-f ${HOME}\".replace(\"\\n\", \"\"))\n",
      "    .add(Namespace.SELECTOR, key=\"pegasus\", value=\"/etc/grid-security/glidein.conf\")\n",
      ";\n",
      "    shared_scratch = SharedScratch(shared_scrach_path)\n",
      "        .add('--shared', '${SHARED_SCRACH}'.replace('\\$', '$'))\n",
      "        ;\n",
      "    local_storage = LocalStorage(local_storage_path);\n",
      "    condorpool = CondorPool(condorpool_path, namespace='CONDOR')\n",
      "        //.add('+SingularityImage', '/cvmfs/singularity.opensciencegrid.org/openscienegridvo6' + '?image_site_catalog=${SINGULARITY_IMAGE_CATALOG}'.split(' ', 2)[0].replace('/', '//').replace('.tar', '.sif'));\n",
      "        )\n",
      "        );\n",
      "\n",
      "\n",
      "job.set_inputs([shared_scratch, submit_directories, condorpoo])\n",
      "job._add_profile(NamespaceKey.ENV, key='PEGASUS_INITIAL_DIR', value='${HOME}'.unetscape());\n",
      "job.__add_profiles__(NamespaceKey['SELECTOR'], key='pegasys', value=`/etc/grids-security/${GLIDEIN_CONF}`.unetescape()`);\n",
      "job__.add_inputs__([sharedscratch, submitedirectories, conddorpool]);\n",
      "job_.add_arguments(`--shared ${SHAREED_SCRACTH}`.unetspace().replace('\"\n"
     ]
    }
   ],
   "source": [
    "#best output\n",
    "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(\"Generated text:\")\n",
    "print(generated_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "finetune",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
