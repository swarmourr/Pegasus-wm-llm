{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    LlamaTokenizer, \n",
    "    LlamaForCausalLM, \n",
    "    TrainingArguments, \n",
    "    Trainer\n",
    ")\n",
    "batch_size = 1024 # Adjust this based on your memory capacity\n",
    "# Load data from JSON file\n",
    "def load_data(file_path):\n",
    "    data = []\n",
    "    with open(file_path, 'r') as f:\n",
    "        items = json.load(f)\n",
    "        for item in items:\n",
    "            text = f\"Instruction: {item['instruction']}\\nResponse: {item['response']}\"\n",
    "            data.append(text)\n",
    "    return data\n",
    "\n",
    "# Prepare dataset\n",
    "def prepare_dataset(data):\n",
    "    return Dataset.from_dict({\"text\": data})\n",
    "\n",
    "def tokenize_data(examples):\n",
    "    tokenized_inputs = tokenizer(examples[\"text\"], truncation=True, padding=\"max_length\", max_length=1024)\n",
    "    tokenized_inputs[\"labels\"] = tokenized_inputs[\"input_ids\"].copy()\n",
    "    return tokenized_inputs\n",
    "\n",
    "# Load model and tokenizer\n",
    "\n",
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-v0.3\",use_auth_token=\"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\")\n",
    "# Add padding token if not present\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.add_special_tokens({'pad_token': tokenizer.eos_token})\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-v0.3\",use_auth_token=\"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\")\n",
    "\n",
    "# Load and prepare data\n",
    "print(\"--> prepare the data\")\n",
    "data = load_data(\"data.json\")\n",
    "dataset = prepare_dataset(data)\n",
    "tokenized_dataset = dataset.map(tokenize_data, batched=True, batch_size=batch_size, num_proc=4)\n",
    "\n",
    "# Set up training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=4,\n",
    "    save_steps=10_000,\n",
    "    save_total_limit=2,\n",
    "    learning_rate=2e-5,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',  # directory for storing logs\n",
    "    logging_steps=200,     # log every 200 steps\n",
    ")\n",
    "\n",
    "# Initialize Trainer\n",
    "print(\"-->  init training\")\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    ")\n",
    "\n",
    "print(\"-->  start training\")\n",
    "# Start training\n",
    "trainer.train()\n",
    "\n",
    "# Save the model\n",
    "print(\"-->  saving the model\")\n",
    "trainer.save_model(\"./finetuned_llama3larger\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForCausalLM, \n",
    "    TrainingArguments, \n",
    "    Trainer\n",
    ")\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "batch_size = 1024  # Adjust this based on your memory capacity\n",
    "\n",
    "# Load data from JSON file\n",
    "def load_data(file_path):\n",
    "    data = []\n",
    "    with open(file_path, 'r') as f:\n",
    "        items = json.load(f)\n",
    "        for item in items:\n",
    "            text = f\"Instruction: {item['instruction']}\\nResponse: {item['response']}\"\n",
    "            data.append(text)\n",
    "    return data\n",
    "\n",
    "# Prepare dataset\n",
    "def prepare_dataset(data):\n",
    "    return Dataset.from_dict({\"text\": data})\n",
    "\n",
    "# Tokenize data\n",
    "def tokenize_data(examples):\n",
    "    tokenized_inputs = tokenizer(examples[\"text\"], truncation=True, padding=\"max_length\", max_length=1024)\n",
    "    tokenized_inputs[\"labels\"] = tokenized_inputs[\"input_ids\"].copy()\n",
    "    return tokenized_inputs\n",
    "\n",
    "# Load model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-v0.3\", use_auth_token=\"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\")\n",
    "# Add padding token if not present\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.add_special_tokens({'pad_token': tokenizer.eos_token})\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-v0.3\", use_auth_token=\"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\")\n",
    "\n",
    "# Move model to the appropriate device\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Load and prepare data\n",
    "print(\"--> Prepare the data\")\n",
    "data = load_data(\"data.json\")\n",
    "dataset = prepare_dataset(data)\n",
    "tokenized_dataset = dataset.map(tokenize_data, batched=True, batch_size=batch_size, num_proc=4)\n",
    "\n",
    "# Set up training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=4,\n",
    "    save_steps=10_000,\n",
    "    save_total_limit=2,\n",
    "    learning_rate=2e-5,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',  # directory for storing logs\n",
    "    logging_steps=200,     # log every 200 steps\n",
    ")\n",
    "\n",
    "# Initialize Trainer\n",
    "print(\"--> Init training\")\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    ")\n",
    "\n",
    "print(\"--> Start training\")\n",
    "# Start training\n",
    "trainer.train()\n",
    "\n",
    "# Save the model\n",
    "print(\"--> Saving the model\")\n",
    "trainer.save_model(\"./llamaLarger\")\n",
    "\n",
    "print(\"--> Saved the model\")\n",
    "\n",
    "# Function to generate text with tqdm progress bar\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling,\n",
    ")\n",
    "\n",
    "# Function to load data from JSON file\n",
    "def load_data(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    return data\n",
    "\n",
    "# Function to prepare dataset\n",
    "def prepare_dataset(data):\n",
    "    texts = [f\"Instruction: {item['instruction']}\\nResponse: {item['response']}\" for item in data]\n",
    "    return Dataset.from_dict({\"text\": texts})\n",
    "\n",
    "# Function to tokenize data\n",
    "def tokenize_data(examples, tokenizer):\n",
    "    model_inputs = tokenizer(examples[\"text\"], truncation=True, padding=\"max_length\", max_length=1024)\n",
    "    model_inputs[\"labels\"] = model_inputs[\"input_ids\"].copy()\n",
    "    return model_inputs\n",
    "\n",
    "# Initialize tokenizer and model\n",
    "def initialize_model_and_tokenizer(model_name):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    # Add padding token if not present\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.add_special_tokens({'pad_token': tokenizer.eos_token})\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name, attn_implementation=\"eager\")\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "    return tokenizer, model\n",
    "\n",
    "# Function to define the training arguments\n",
    "def define_training_args(output_dir=\"./results\"):\n",
    "    return TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        overwrite_output_dir=True,\n",
    "        num_train_epochs=1,\n",
    "        per_device_train_batch_size=2,\n",
    "        gradient_accumulation_steps=8,\n",
    "        save_steps=10_000,\n",
    "        save_total_limit=2,\n",
    "        learning_rate=2e-5,\n",
    "        warmup_steps=500,\n",
    "        weight_decay=0.01,\n",
    "        logging_dir='./logs',\n",
    "        logging_steps=200,\n",
    "        use_mps_device=True,\n",
    "    )\n",
    "\n",
    "# Function to define the Trainer\n",
    "def create_trainer(model, args, train_dataset, tokenizer):\n",
    "    data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "    return Trainer(\n",
    "        model=model,\n",
    "        args=args,\n",
    "        train_dataset=train_dataset,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "    )\n",
    "\n",
    "# Function to fine-tune and train the model\n",
    "def train_model(trainer, output_dir):\n",
    "    trainer.train()\n",
    "    trainer.save_model(output_dir)\n",
    "\n",
    "# Main function to orchestrate the training process\n",
    "def main():\n",
    "    # Set the device\n",
    "    device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # Initialize model and tokenizer\n",
    "    model_name = \"ybelkada/falcon-7b-sharded-bf16\"\n",
    "    tokenizer, model = initialize_model_and_tokenizer(model_name)\n",
    "\n",
    "    # Move model to the appropriate device\n",
    "    model.to(device)\n",
    "\n",
    "    # Load and prepare data\n",
    "    data = load_data(\"data.json\")\n",
    "    dataset = prepare_dataset(data)\n",
    "    tokenized_dataset = dataset.map(\n",
    "        lambda x: tokenize_data(x, tokenizer),\n",
    "        batched=True,\n",
    "        num_proc=4,\n",
    "        remove_columns=dataset.column_names\n",
    "    )\n",
    "\n",
    "    # Define training arguments\n",
    "    training_args = define_training_args()\n",
    "\n",
    "    # Create Trainer\n",
    "    trainer = create_trainer(model, training_args, tokenized_dataset, tokenizer)\n",
    "\n",
    "    # Train model\n",
    "    output_dir = \"./llamaLarger1\"\n",
    "    train_model(trainer, output_dir)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(prompts, model, tokenizer, device, temperature=0.7, max_length=200):\n",
    "    model.eval()\n",
    "    generated_texts = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for prompt in tqdm(prompts, desc=\"Generating texts\"):\n",
    "            inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "            output = model.generate(**inputs, max_length=max_length, temperature=temperature)\n",
    "            generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "            generated_texts.append(generated_text)\n",
    "    \n",
    "    return generated_texts\n",
    "\n",
    "# Example usage of the generate_text function\n",
    "prompts = [\"create a complete pegasus workflow using python for machine learning\"]\n",
    "generated_texts = generate_text(prompts, model, tokenizer, device)\n",
    "\n",
    "for text in generated_texts:\n",
    "    print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type mistral to instantiate a model of type llama. This is not supported for all configurations of models and can yield errors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer loaded.\n",
      "Loading model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44e8cd2d6d8a4534907aefc35918284c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import LlamaTokenizer, LlamaForCausalLM\n",
    "\n",
    "# Specify the paths to the saved model and tokenizer\n",
    "model_path = \"./finetuned_mistralLarger\"\n",
    "#tokenizer_path = \"pthornton614/CodeLama-7b-Instruct\"  # Same as the path used during fine-tuning\n",
    "tokenizer_path = \"mistralai/Mistral-7B-v0.3\"  # Same as the path used during fine-tuning\n",
    "\n",
    "# Load the tokenizer\n",
    "print(\"Loading tokenizer...\")\n",
    "tokenizer = LlamaTokenizer.from_pretrained(tokenizer_path)\n",
    "\n",
    "# Set the padding token\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print(\"Tokenizer loaded.\")\n",
    "\n",
    "# Load the model\n",
    "print(\"Loading model...\")\n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "model = LlamaForCausalLM.from_pretrained(model_path).to(device)\n",
    "print(\"Model loaded.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text...\n",
      "Generating text...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating sequences: 100%|██████████| 1/1 [02:09<00:00, 129.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text:\n",
      "Instruction: Create a complete example of pegasus workflow using the python also with replicas and sites \n",
      "Response: from Pegasus.api import *\n",
      "\n",
      "# Define a workflow\n",
      "wf = Workflow(\"replica-example\")\n",
      "\n",
      "# Add a job with a replica\n",
      "job = Job(\"process\")\n",
      "job.add_inputs(File(\"raw_data.txt\"))\n",
      "job.add_args(File(\"raw_data.txt\"))\n",
      "job.add_args(File(\"processed_data.txt\"))\n",
      "job.add_args(File(\"processed_data.txt\"))\n",
      "job.add_args(File(\"aggregated_data.txt\"))\n",
      "job.add_args(File(\"aggregated_data.txt\"))\n",
      "job.add_args(File(\"results.txt\"))\n",
      "job.add_args(File(\"results.txt\"))\n",
      "job.add_args(File(\"summary.txt\"))\n",
      "job.add_args(File(\"summary.txt\"))\n",
      "job.add_args(File(\"final_results.txt\"))\n",
      "job.add_args(File(\"final_results.txt\"))\n",
      "job.add_args(File(\"final_summary.txt\"))\n",
      "job.add_args(File(\"final_summary.txt\"))\n",
      "job.add_args(File(\"final_results_with_planning.txt\"))\n",
      "job.add_args(File(\"final_results_with_planning.txt\"))\n",
      "job.add_args(File(\"final_summary_with_planning.txt\"))\n",
      "job.add_args(File(\"final_summary_with_planning.txt\"))\n",
      "job.add_args(File(\"final_results_with_planning_and_replica.txt\"))\n",
      "job.add_args(File(\"final_results_with_planning_and_replica.txt\"))\n",
      "job.add_args(File(\"final_summary_with_planning_and_replica.txt\"))\n",
      "job.add_args(File(\"final_summary_with_planning_and_replica.txt\"))\n",
      "job.add_args(File(\"final_results_with_planning_and_replica_and_site.txt\"))\n",
      "job.add_args(File(\"final_\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Prepare a prompt\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "prompt = \"Instruction: Create a complete example of pegasus workflow using the python also with replicas and sites \\nResponse:\"\n",
    "\n",
    "# Tokenize the prompt and provide attention mask\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=False,max_length=500)\n",
    "input_ids = inputs.input_ids.to(device)\n",
    "attention_mask = inputs.attention_mask.to(device)\n",
    "\n",
    "# Generate text\n",
    "print(\"Generating text...\")\n",
    "# Generate text with diverse decoding strategies\n",
    "\n",
    "\n",
    "print(\"Generating text...\")\n",
    "with torch.no_grad():\n",
    "    for _ in tqdm(range(1), desc=\"Generating sequences\"):\n",
    "        outputs = model.generate(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            max_length=500,\n",
    "            num_return_sequences=1,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            temperature = 0.3# Adjust the temperature value as needed\n",
    "\n",
    "        )\n",
    "# Decode and print the generated text\n",
    "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(\"Generated text:\")\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text...\n",
      "Generating text...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating sequences: 100%|██████████| 1/1 [01:12<00:00, 72.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text:\n",
      "Instruction: Convert the following SLURM script into a Pegasus workflow using the Pegasus.api Python package. The workflow should replicate the functionality of the SLURM script as closely as possible, including file handling, job execution, and cleanup. #!/bin/bash\n",
      "#SBATCH --job-name=array_job\n",
      "#SBATCH --output=array_job_%A_%a.out\n",
      "#SBATCH --error=array_job_%A_%a.err\n",
      "#SBATCH --array=0-9\n",
      "\n",
      "# Define and create a unique scratch directory\n",
      "SCRATCH_DIRECTORY=/global/work/${USER}/job-array-example/${SLURM_JOBID}\n",
      "mkdir -p ${SCRATCH_DIRECTORY}\n",
      "cd ${SCRATCH_DIRECTORY}\n",
      "\n",
      "cp ${SLURM_SUBMIT_DIR}/test.py ${SCRATCH_DIRECTORY}\n",
      "\n",
      "# Each job will see a different ${SLURM_ARRAY_TASK_ID}\n",
      "echo \"now processing task id:: ${SLURM_ARRAY_TASK_ID}\"\n",
      "python test.py > output_${SLURM_ARRAY_TASK_ID}.txt\n",
      "\n",
      "# After the job is done we copy our output back to ${SLURM_SUBMIT_DIR}\n",
      "cp output_${SLURM_ARRAY_TASK_ID}.txt ${SLURM_SUBMIT_DIR}\n",
      "\n",
      "# We step out of the scratch directory and remove it\n",
      "cd ${SLURM_SUBMIT_DIR}\n",
      "rm -rf ${SCRATCH_DIRECTORY}\n",
      "\n",
      "# Happy end\n",
      "exit 0\n",
      "\n",
      "Response: from Pegasus.api import *\n",
      "\n",
      "# Define workflow and job\n",
      "array_workflow = Workflow(\"array_workflow\")\n",
      "job = Job(\"process\")\n",
      "args = f\"--submit-directory={PEGASUS_SUBDIT_DIR} --scratch-directory={PEGASUS_SUBDIT_DIR} --workflow={WORKFLOW_NAME} --job-name={JOB_NAME} --output-directory={WORK_DIR} --input-files={TRAIN_DATA} --model-files={MODEL_DIR} --data-files={DATA_DIR}\".split()\n",
      "job.add_args(*args)\n",
      "array_workflow.add_job(job)\n",
      "\n",
      "# Write the workflow to a file\n",
      "array_workflow.write()\n",
      "\n",
      "# Submit the workflow to start\n",
      "submit_directory = f\"{HOME}\".replace(\"\\\\\", \"\\\\\\\\\")\n",
      "submit_directory = f\"/Volumes/{submit_directory}\".replace(\"\\\\\", \"\\\\\\\\\")\n",
      "submit_directory = f\"/Users/maciej/Work/{submit_directory}\".replace(\"\\\\\", \"\\\\\\\\\")\n",
      "submit_directory = \"/Volumes/Maciej/Work/\"\n",
      "\n",
      "mkdir_command = f\"mkdir -p ${submit_directory} ${HOME}/bin ${HOME}/.bosco\"\n",
      "\n",
      "mkdir_job = Job(mkdir_command)\n",
      "mkdir_job.submit()\n",
      "\n",
      "stash_home = f\"{submit_directory}StashHome\"\n",
      "\n",
      "transfer_job = Job(transfer_job_name, _id=TransferJobName)\n",
      "transfer_job.add_args(submit_directory, stash_home)\n",
      "transfer_job.add_inputs(files_glob)\n",
      "transfer_job.add_outputs(submit_directory, stash_home)\n",
      "transfer_job.submit()\n",
      "\n",
      "execute_workflow = Workflow(SubWorkFlow)\n",
      "execute_workflow.write()\n",
      "subworkflow_stager_job = Job(StageSubWorkFlowToStashHome)\n",
      "subworkflow_stager_job.add_args(submit_directory, stash_home)\n",
      "subworkflow_stager_job.add_inputs(TransferJobName, SubWorkflow.get_full_name())\n",
      "subworkflow_stager_job.add_outputs(submit_directory, stash_home)\n",
      "subworkflow_stager_job.submit()\n",
      "\n",
      "analyze_job = Job(SubWorkFlow, wait_time=1*60, output_dir=submit_directory, subworkflow_name=EXECUTABLE_NAME, use_condor_pool=True, require_grid_shifter=True)\n",
      "analyze_job.add_args(submit_directory, stash_home, files_glob\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Prepare a prompt\n",
    "from tqdm import tqdm\n",
    "\n",
    "slurm_script = \"\"\"#!/bin/bash\n",
    "#SBATCH --job-name=array_job\n",
    "#SBATCH --output=array_job_%A_%a.out\n",
    "#SBATCH --error=array_job_%A_%a.err\n",
    "#SBATCH --array=0-9\n",
    "\n",
    "# Define and create a unique scratch directory\n",
    "SCRATCH_DIRECTORY=/global/work/${USER}/job-array-example/${SLURM_JOBID}\n",
    "mkdir -p ${SCRATCH_DIRECTORY}\n",
    "cd ${SCRATCH_DIRECTORY}\n",
    "\n",
    "cp ${SLURM_SUBMIT_DIR}/test.py ${SCRATCH_DIRECTORY}\n",
    "\n",
    "# Each job will see a different ${SLURM_ARRAY_TASK_ID}\n",
    "echo \"now processing task id:: ${SLURM_ARRAY_TASK_ID}\"\n",
    "python test.py > output_${SLURM_ARRAY_TASK_ID}.txt\n",
    "\n",
    "# After the job is done we copy our output back to ${SLURM_SUBMIT_DIR}\n",
    "cp output_${SLURM_ARRAY_TASK_ID}.txt ${SLURM_SUBMIT_DIR}\n",
    "\n",
    "# We step out of the scratch directory and remove it\n",
    "cd ${SLURM_SUBMIT_DIR}\n",
    "rm -rf ${SCRATCH_DIRECTORY}\n",
    "\n",
    "# Happy end\n",
    "exit 0\n",
    "\"\"\"\n",
    "\n",
    "prompt = f\"Instruction: Convert the following SLURM script into a Pegasus workflow using the Pegasus.api Python package. The workflow should replicate the functionality of the SLURM script as closely as possible, including file handling, job execution, and cleanup. {slurm_script}\\nResponse:\"\n",
    "\n",
    "# Tokenize the prompt and provide attention mask\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True,max_length=1000)\n",
    "input_ids = inputs.input_ids.to(device)\n",
    "attention_mask = inputs.attention_mask.to(device)\n",
    "\n",
    "# Generate text\n",
    "print(\"Generating text...\")\n",
    "# Generate text with diverse decoding strategies\n",
    "\n",
    "\n",
    "print(\"Generating text...\")\n",
    "with torch.no_grad():\n",
    "    for _ in tqdm(range(1), desc=\"Generating sequences\"):\n",
    "        outputs = model.generate(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            max_length=1000,\n",
    "            num_return_sequences=1,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            top_k=50,\n",
    "            do_sample=True\n",
    "        )\n",
    "# Decode and print the generated text\n",
    "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(\"Generated text:\")\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"\"\"Instruction: Convert the following SLURM script into a Pegasus workflow using the Pegasus.api Python package. \n",
    "Your response should be a complete Python script that includes:\n",
    "1. Necessary imports from Pegasus.api\n",
    "2. Workflow creation\n",
    "3. Job definitions that replicate the SLURM script functionality\n",
    "4. Transformation, Site, and Replica catalogs as needed\n",
    "5. Writing the workflow to a file\n",
    "\n",
    "SLURM script:\n",
    "{slurm_script}\n",
    "\n",
    "Begin your response with 'from Pegasus.api import *' and end it with 'workflow.write()'.\n",
    "Do not repeat code unnecessarily. Ensure each part of the SLURM script functionality is addressed only once in the Pegasus workflow.\n",
    "\n",
    "Response:\n",
    "\"\"\"\n",
    "outputs = model.generate(\n",
    "    input_ids,\n",
    "    attention_mask=attention_mask,\n",
    "    max_length=1000,  # Increase max length for longer outputs\n",
    "    num_return_sequences=1,\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    "    do_sample=True,  # Enable sampling\n",
    "    temperature=0.2,  # Adjust temperature (lower for more focused outputs)\n",
    "    top_k=50,  # Limit to top k tokens\n",
    "    top_p=0.95,  # Nucleus sampling\n",
    "    no_repeat_ngram_size=3,  # Prevent repetition of 3-grams\n",
    "    early_stopping=True,\n",
    "    repetition_penalty=1.2  # Penalize repetition\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text:\n",
      "Instruction: Convert the following SLURM script into a Pegasus workflow using the Pegasus.api Python package. The workflow should replicate the functionality of the SLURM script as closely as possible, including file handling, job execution, and cleanup. #!/bin/bash\n",
      "#SBATCH --job-name=array_job\n",
      "#SBATCH --output=array_job_%A_%a.out\n",
      "#SBATCH --error=array_job_%A_%a.err\n",
      "#SBATCH --array=0-9\n",
      "\n",
      "# Define and create a unique scratch directory\n",
      "SCRATCH_DIRECTORY=/global/work/${USER}/job-array-example/${SLURM_JOBID}\n",
      "mkdir -p ${SCRATCH_DIRECTORY}\n",
      "cd ${SCRATCH_DIRECTORY}\n",
      "\n",
      "cp ${SLURM_SUBMIT_DIR}/test.py ${SCRATCH_DIRECTORY}\n",
      "\n",
      "# Each job will see a different ${SLURM_ARRAY_TASK_ID}\n",
      "echo \"now processing task id:: ${SLURM_ARRAY_TASK_ID}\"\n",
      "python test.py > output_${SLURM_ARRAY_TASK_ID}.txt\n",
      "\n",
      "# After the job is done we copy our output back to ${SLURM_SUBMIT_DIR}\n",
      "cp output_${SLURM_ARRAY_TASK_ID}.txt ${SLURM_SUBMIT_DIR}\n",
      "\n",
      "# We step out of the scratch directory and remove it\n",
      "cd ${SLURM_SUBMIT_DIR}\n",
      "rm -rf ${SCRATCH_DIRECTORY}\n",
      "\n",
      "# Happy end\n",
      "exit 0\n",
      "\n",
      "Response: from Pegasu.api import *\n",
      "\n",
      "wf = Workflow(\"converted-slurm\")\n",
      "\n",
      "for i in range(10):\n",
      "    job = Job(process)\n",
      "    input_files = File(File(job.get_input()[0]))\n",
      "    output_files.append(File(\"results_{}\".format(i)))\n",
      "    wf.add_jobs(job, args=[input_files])\n",
      "\n",
      "    # Adding files for each job\n",
      "    process_file = File(\"process.py\")\n",
      "    data_file     = File(\"/Volumes/ryan/data.csv\")\n",
      "                            .add_metadata(creator=\"ryan\", date=Date.today())\n",
      "    )\n",
      "    results_file   = File(\"./*\").add_regex_replica(\"local\", \"/Users/ryantanaka/Documents/Pegasus/[0]\".format(__FILE__))\n",
      "                                .add(\"remote\", \"/home/rytanaka/Work/Papajimmy/[1]\".replace(\"\\\\\", \"/\"))\n",
      "                              )\n",
      "                          );\n",
      "    submit_directory = Directory(submit_directory_path).add_args(\"-f ${HOME}\".replace(\"\\n\", \"\"))\n",
      "    .add(Namespace.SELECTOR, key=\"pegasus\", value=\"/etc/grid-security/glidein.conf\")\n",
      ";\n",
      "    shared_scratch = SharedScratch(shared_scrach_path)\n",
      "        .add('--shared', '${SHARED_SCRACH}'.replace('\\$', '$'))\n",
      "        ;\n",
      "    local_storage = LocalStorage(local_storage_path);\n",
      "    condorpool = CondorPool(condorpool_path, namespace='CONDOR')\n",
      "        //.add('+SingularityImage', '/cvmfs/singularity.opensciencegrid.org/openscienegridvo6' + '?image_site_catalog=${SINGULARITY_IMAGE_CATALOG}'.split(' ', 2)[0].replace('/', '//').replace('.tar', '.sif'));\n",
      "        )\n",
      "        );\n",
      "\n",
      "\n",
      "job.set_inputs([shared_scratch, submit_directories, condorpoo])\n",
      "job._add_profile(NamespaceKey.ENV, key='PEGASUS_INITIAL_DIR', value='${HOME}'.unetscape());\n",
      "job.__add_profiles__(NamespaceKey['SELECTOR'], key='pegasys', value=`/etc/grids-security/${GLIDEIN_CONF}`.unetescape()`);\n",
      "job__.add_inputs__([sharedscratch, submitedirectories, conddorpool]);\n",
      "job_.add_arguments(`--shared ${SHAREED_SCRACTH}`.unetspace().replace('\"\n"
     ]
    }
   ],
   "source": [
    "#best  Decode and print the generated text\n",
    "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(\"Generated text:\")\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text:\n",
      "Instruction: Can you write Pegasus workflow using the Pegasus.api Python package for Federated Learning. \n",
      "Your response should be a complete Python script that includes:\n",
      "1. Necessary imports from Pegasus.api\n",
      "2. Workflow creation\n",
      "3. Job definitions that replicate the SLURM script functionality\n",
      "4. Transformation, Site, and Replica catalogs as needed\n",
      "5. Writing the workflow to a file\n",
      "\n",
      "Begin your response with 'from Pegasus.api import *' and end it with 'workflow.write()'.\n",
      "Do not repeat code unnecessarily. Ensure each part of the SLURM script functionality is addressed only once in the Pegasus workflow.\n",
      "\n",
      "Response:\n",
      "# --- Write the work flow to a DAG file ---\n",
      "wf = WorkFlow(\"diamond-cluster\", infer_dependencies=True)\n",
      "job = (Job(submitdir='/home/ryan/Work', cmd=top_directory / \"bin/pegasus-kickstart\", args=args[0]) & check_inputs(File('in.txt'), File('intermediate.txt')) >> output_files[0] & input_files[-1]\n",
      "job.add_args(*args, **kwargs)\n",
      "logfile = LogFile(__name__)\n",
      "job += add_monitord(directory=shared_scratch, profile={'stdout': logfile}, event_type={'end'}).add_inputs(*input_files).add_outputs(*output_files)\n",
      "wmsgs = [Namespace.SELECTOR, Namespace.CONDOR, '+SingularityImage', 'condor://osgconnect/cvmfs/singularity.opensciencegrid.org', '5.1']\n",
      "job *= add_pegasushub_profile().add_globus_profile(wmsgas=' '.join([i for i in wmsgs]), gpgkey='', keychain='')\n",
      "wftxt = workflow.__eq__(\"\") and \"Workflow name not specified\" or \"<pre>\\n{WORKFLOW}\".format(WORKFLOG)) + \"</pre>\".replace(\"\\r\\n\",\"<br>\")\n",
      "wif = WorkflowInfo(name=wft.name, arch=Arch.X86_64, osrelease='', osversion='', dir='/tmp')\n",
      "with WMSConfig(label='local', wms.size='small').set_env('PEGASUS_HOME', '/Volumes/data')\n",
      ".add_(Namespace.CONDOR, value='local')\n",
      "                              .add_pegashub_profiles(login_username='ryan', contact_email='ryann@isi.edu')\n",
      "        )\n",
      "    );\n",
      "    ));\n",
      ");\n"
     ]
    }
   ],
   "source": [
    "prompt = f\"\"\"Instruction: Can you write Pegasus workflow using the Pegasus.api Python package for Federated Learning. \n",
    "Your response should be a complete Python script that includes:\n",
    "1. Necessary imports from Pegasus.api\n",
    "2. Workflow creation\n",
    "3. Job definitions that replicate the SLURM script functionality\n",
    "4. Transformation, Site, and Replica catalogs as needed\n",
    "5. Writing the workflow to a file\n",
    "\n",
    "Begin your response with 'from Pegasus.api import *' and end it with 'workflow.write()'.\n",
    "Do not repeat code unnecessarily. Ensure each part of the SLURM script functionality is addressed only once in the Pegasus workflow.\n",
    "\n",
    "Response:\n",
    "\"\"\"\n",
    "\n",
    "# Tokenize the prompt and provide attention mask\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True,max_length=1000)\n",
    "input_ids = inputs.input_ids.to(device)\n",
    "attention_mask = inputs.attention_mask.to(device)\n",
    "\n",
    "outputs = model.generate(\n",
    "    input_ids,\n",
    "    attention_mask=attention_mask,\n",
    "    max_length=1000,  # Increase max length for longer outputs\n",
    "    num_return_sequences=1,\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    "    do_sample=True,  # Enable sampling\n",
    "    temperature=0.8,  # Adjust temperature (lower for more focused outputs)\n",
    "    top_k=50,  # Limit to top k tokens\n",
    "    top_p=0.95,  # Nucleus sampling\n",
    "    no_repeat_ngram_size=3,  # Prevent repetition of 3-grams\n",
    "    early_stopping=True,\n",
    "    repetition_penalty=1.2  # Penalize repetition\n",
    ")\n",
    "# Decode and print the generated text\n",
    "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(\"Generated text:\")\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#repare a prompt\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "prompt = \"Instruction:how to submit the pegasus workflows\\nResponse:\"\n",
    "\n",
    "# Tokenize the prompt and provide attention mask\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True,max_length=1000)\n",
    "input_ids = inputs.input_ids.to(device)\n",
    "attention_mask = inputs.attention_mask.to(device)\n",
    "\n",
    "# Generate text\n",
    "print(\"Generating text...\")\n",
    "# Generate text with diverse decoding strategies\n",
    "\n",
    "\n",
    "print(\"Generating text...\")\n",
    "with torch.no_grad():\n",
    "    for _ in tqdm(range(1), desc=\"Generating sequences\"):\n",
    "        outputs = model.generate(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            max_length=1000,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            temperature = 0.3 # Adjust the temperature value as needed\n",
    "\n",
    "        )\n",
    "# Decode and print the generated text\n",
    "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(\"Generated text:\")\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Define dummy input for the model (adjust dimensions as necessary)\n",
    "dummy_input = torch.randint(0, 50256, (1, 1024))  # Replace with actual vocab size and input length\n",
    "\n",
    "# Export the model to ONNX format\n",
    "onnx_model_path = \"model.onnx\"\n",
    "torch.onnx.export(model, dummy_input, onnx_model_path, \n",
    "                  input_names=['input_ids'], output_names=['logits'], \n",
    "                  dynamic_axes={'input_ids': {0: 'batch_size', 1: 'sequence_length'}, 'logits': {0: 'batch_size', 1: 'sequence_length'}})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling\n",
    ")\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "import os\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Constants\n",
    "BATCH_SIZE = 8  # Adjust this based on your memory capacity\n",
    "MAX_LENGTH = 1024\n",
    "MODEL_NAME = \"google/gemma-2-9b\"\n",
    "AUTH_TOKEN = \"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\"\n",
    "OUTPUT_DIR = \"./results\"\n",
    "MODEL_SAVE_PATH = \"./gemmaLarger\"\n",
    "\n",
    "def load_data(file_path):\n",
    "    \"\"\"Load data from JSON file.\"\"\"\n",
    "    with open(file_path, 'r') as f:\n",
    "        items = json.load(f)\n",
    "    return [f\"Instruction: {item['instruction']}\\nResponse: {item['response']}\" for item in items]\n",
    "\n",
    "def prepare_dataset(data):\n",
    "    \"\"\"Prepare dataset from data.\"\"\"\n",
    "    return Dataset.from_dict({\"text\": data})\n",
    "\n",
    "def tokenize_data(examples, tokenizer):\n",
    "    \"\"\"Tokenize data.\"\"\"\n",
    "    tokenized_inputs = tokenizer(examples[\"text\"], truncation=True, padding=\"max_length\", max_length=MAX_LENGTH)\n",
    "    tokenized_inputs[\"labels\"] = tokenized_inputs[\"input_ids\"].copy()\n",
    "    return tokenized_inputs\n",
    "\n",
    "def setup_model_and_tokenizer():\n",
    "    \"\"\"Set up model and tokenizer.\"\"\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_auth_token=AUTH_TOKEN)\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.add_special_tokens({'pad_token': tokenizer.eos_token})\n",
    "    \n",
    "    model = AutoModelForCausalLM.from_pretrained(MODEL_NAME, use_auth_token=AUTH_TOKEN, trust_remote_code=True)\n",
    "    \n",
    "    if torch.backends.mps.is_available():\n",
    "        device = torch.device(\"mps\")\n",
    "        logger.info(\"Using MPS (Metal Performance Shaders) device\")\n",
    "    elif torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "        logger.info(\"Using CUDA device\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "        logger.info(\"Using CPU\")\n",
    "    \n",
    "    model.to(\"mps\")\n",
    "    # Allocate some dummy data to warm up the model on the device\n",
    "    with torch.no_grad():\n",
    "        dummy_input = torch.ones((1, 1), dtype=torch.int64).to(\"mps\")\n",
    "        _ = model(dummy_input)\n",
    "    \n",
    "    return model, tokenizer, device\n",
    "\n",
    "class MPSTrainer(Trainer):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "    def _prepare_inputs(self, inputs):\n",
    "        \"\"\"\n",
    "        Prepare inputs for the model, moving them to the correct device.\n",
    "        \"\"\"\n",
    "        for k, v in inputs.items():\n",
    "            if isinstance(v, torch.Tensor):\n",
    "                inputs[k] = v.to(\"mps\")\n",
    "        return inputs\n",
    "    \n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        \"\"\"\n",
    "        Custom compute_loss function to ensure loss is computed on the correct device.\n",
    "        \"\"\"\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        # Move labels to the same device as logits\n",
    "        labels = labels.to(\"mps\")\n",
    "        loss_fct = torch.nn.CrossEntropyLoss()\n",
    "        loss = loss_fct(logits.view(-1, logits.size(-1)), labels.view(-1))\n",
    "        loss = loss.cpu()\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "def main():\n",
    "    model, tokenizer, device = setup_model_and_tokenizer()\n",
    "\n",
    "    logger.info(\"Preparing the data\")\n",
    "    data = load_data(\"data.json\")  # Ensure this file path is correct\n",
    "    dataset = prepare_dataset(data)\n",
    "    tokenized_dataset = dataset.map(\n",
    "        lambda examples: tokenize_data(examples, tokenizer),\n",
    "        batched=True,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        num_proc=os.cpu_count(),\n",
    "        remove_columns=[\"text\"]\n",
    "    )\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=OUTPUT_DIR,\n",
    "        num_train_epochs=1,\n",
    "        per_device_train_batch_size=4,\n",
    "        save_steps=10_000,\n",
    "        save_total_limit=2,\n",
    "        learning_rate=2e-5,\n",
    "        warmup_steps=500,\n",
    "        weight_decay=0.01,\n",
    "        logging_dir='./logs',\n",
    "        logging_steps=200,\n",
    "        gradient_accumulation_steps=4,\n",
    "        eval_steps=1000,\n",
    "        evaluation_strategy=\"steps\",\n",
    "        load_best_model_at_end=True,\n",
    "        use_mps_device=True\n",
    "    )\n",
    "\n",
    "    data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "    logger.info(\"Initializing trainer\")\n",
    "    trainer = MPSTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_dataset,\n",
    "        data_collator=data_collator,\n",
    "    )\n",
    "\n",
    "    logger.info(\"Starting training\")\n",
    "    trainer.train()\n",
    "\n",
    "    logger.info(f\"Saving the model to {MODEL_SAVE_PATH}\")\n",
    "    trainer.save_model(MODEL_SAVE_PATH)\n",
    "    logger.info(\"Model saved successfully\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling\n",
    ")\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "import os\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Constants\n",
    "BATCH_SIZE = 32  # Reduced batch size for CPU training\n",
    "MAX_LENGTH = 1024\n",
    "MODEL_NAME = \"google/gemma-2-9b\"\n",
    "AUTH_TOKEN = \"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\"\n",
    "OUTPUT_DIR = \"./results\"\n",
    "MODEL_SAVE_PATH = \"./gemmaLarger\"\n",
    "\n",
    "def load_data(file_path):\n",
    "    \"\"\"Load data from JSON file.\"\"\"\n",
    "    with open(file_path, 'r') as f:\n",
    "        items = json.load(f)\n",
    "    return [f\"Instruction: {item['instruction']}\\nResponse: {item['response']}\" for item in items]\n",
    "\n",
    "def prepare_dataset(data):\n",
    "    \"\"\"Prepare dataset from data.\"\"\"\n",
    "    return Dataset.from_dict({\"text\": data})\n",
    "\n",
    "def tokenize_data(examples, tokenizer):\n",
    "    \"\"\"Tokenize data.\"\"\"\n",
    "    tokenized_inputs = tokenizer(examples[\"text\"], truncation=True, padding=\"max_length\", max_length=MAX_LENGTH)\n",
    "    tokenized_inputs[\"labels\"] = tokenized_inputs[\"input_ids\"].copy()\n",
    "    return tokenized_inputs\n",
    "\n",
    "def setup_model_and_tokenizer():\n",
    "    \"\"\"Set up model and tokenizer.\"\"\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_auth_token=AUTH_TOKEN)\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.add_special_tokens({'pad_token': tokenizer.eos_token})\n",
    "    \n",
    "    model = AutoModelForCausalLM.from_pretrained(MODEL_NAME, use_auth_token=AUTH_TOKEN)\n",
    "    \n",
    "    device = torch.device(\"cpu\")\n",
    "    logger.info(\"Forcing CPU usage\")\n",
    "    \n",
    "    model.to(device)\n",
    "    \n",
    "    return model, tokenizer\n",
    "\n",
    "def main():\n",
    "    model, tokenizer = setup_model_and_tokenizer()\n",
    "\n",
    "    logger.info(\"Preparing the data\")\n",
    "    data = load_data(\"data.json\")\n",
    "    dataset = prepare_dataset(data)\n",
    "    tokenized_dataset = dataset.map(\n",
    "        lambda examples: tokenize_data(examples, tokenizer),\n",
    "        batched=True,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        num_proc=1  # Set to 1 for CPU\n",
    "    )\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=OUTPUT_DIR,\n",
    "        num_train_epochs=1,\n",
    "        per_device_train_batch_size=2,  # Reduced for CPU\n",
    "        save_steps=10_000,\n",
    "        save_total_limit=2,\n",
    "        learning_rate=2e-5,\n",
    "        warmup_steps=500,\n",
    "        weight_decay=0.01,\n",
    "        logging_dir='./logs',\n",
    "        logging_steps=200,\n",
    "        fp16=False,  # Disable fp16\n",
    "        bf16=False,  # Disable bf16\n",
    "        gradient_accumulation_steps=8,  # Increased for CPU\n",
    "        eval_steps=1000,\n",
    "        evaluation_strategy=\"steps\",\n",
    "        load_best_model_at_end=True,\n",
    "        \n",
    "    )\n",
    "\n",
    "    data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "    logger.info(\"Initializing trainer\")\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_dataset,\n",
    "        data_collator=data_collator,\n",
    "    )\n",
    "\n",
    "    logger.info(\"Starting training\")\n",
    "    trainer.train()\n",
    "\n",
    "    logger.info(f\"Saving the model to {MODEL_SAVE_PATH}\")\n",
    "    trainer.save_model(MODEL_SAVE_PATH)\n",
    "    logger.info(\"Model saved successfully\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "finetune",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
